from unittest.mock import patch

from litellm.types.llms.openai import ResponseAPIUsage, ResponsesAPIResponse
from openai.types.responses.response_function_tool_call import ResponseFunctionToolCall
from openai.types.responses.response_output_message import ResponseOutputMessage
from openai.types.responses.response_output_text import ResponseOutputText
from openai.types.responses.response_reasoning_item import (
    ResponseReasoningItem,
    Summary,
)

from openhands.sdk.llm.llm import LLM
from openhands.sdk.llm.message import Message, ReasoningItemModel, TextContent
from openhands.sdk.llm.options.responses_options import select_responses_options


def build_responses_message_output(texts: list[str]) -> ResponseOutputMessage:
    parts = [
        ResponseOutputText(type="output_text", text=t, annotations=[]) for t in texts
    ]
    # Bypass stricter static type expectations in test context; runtime is fine
    return ResponseOutputMessage.model_construct(
        id="m1",
        type="message",
        role="assistant",
        status="completed",
        content=parts,  # type: ignore[arg-type]
    )


def test_from_llm_responses_output_parsing():
    # Build typed Responses output: assistant message text + function call + reasoning
    msg = build_responses_message_output(["Hello", "World"])  # concatenated
    fc = ResponseFunctionToolCall(
        type="function_call", name="do", arguments="{}", call_id="fc_1", id="fc_1"
    )
    reasoning = ResponseReasoningItem(
        id="rid",
        type="reasoning",
        summary=[
            Summary(type="summary_text", text="sum1"),
            Summary(type="summary_text", text="sum2"),
        ],
        content=None,
        encrypted_content=None,
        status="completed",
    )

    m = Message.from_llm_responses_output([msg, fc, reasoning])
    # Assistant text joined
    assert m.role == "assistant"
    assert [c.text for c in m.content if isinstance(c, TextContent)] == ["Hello\nWorld"]
    # Tool call normalized
    assert m.tool_calls and m.tool_calls[0].name == "do"
    # Reasoning mapped
    assert isinstance(m.responses_reasoning_item, ReasoningItemModel)
    assert m.responses_reasoning_item.summary == ["sum1", "sum2"]


def test_normalize_responses_kwargs_policy():
    llm = LLM(model="gpt-5-mini")
    # Use a model that is explicitly Responses-capable per model_features

    # enable encrypted reasoning and set max_output_tokens to test passthrough
    llm.enable_encrypted_reasoning = True
    llm.max_output_tokens = 128

    # Test that passing temperature raises an error
    try:
        select_responses_options(
            llm, {"temperature": 0.3}, include=["text.output_text"], store=None
        )
        assert False, "Expected ValueError when setting temperature"
    except ValueError as e:
        assert "temperature" in str(e).lower()
        assert "not supported" in str(e).lower()

    # Test without temperature - should work fine
    out = select_responses_options(llm, {}, include=["text.output_text"], store=None)
    # Temperature should NOT be set for Responses API models like gpt-5-mini
    assert "temperature" not in out
    assert out["tool_choice"] == "auto"
    # include should contain original and encrypted_content
    assert set(out["include"]) >= {"text.output_text", "reasoning.encrypted_content"}
    # store default to False when None passed
    assert out["store"] is False
    # reasoning config defaulted
    r = out["reasoning"]
    assert r["effort"] in {"low", "medium", "high", "none"}
    assert r["summary"] == "detailed"
    # max_output_tokens preserved
    assert out["max_output_tokens"] == 128


def test_normalize_responses_kwargs_non_reasoning_model():
    llm = LLM(model="codex-mini-latest")
    # Use a non-reasoning Responses-capable model

    llm.max_output_tokens = 256

    # Test that passing temperature raises an error (same as reasoning models)
    try:
        select_responses_options(
            llm, {"temperature": 0.3}, include=["text.output_text"], store=None
        )
        assert False, "Expected ValueError when setting temperature"
    except ValueError as e:
        assert "temperature" in str(e).lower()
        assert "not supported" in str(e).lower()

    # Test without temperature - should work fine
    out = select_responses_options(llm, {}, include=["text.output_text"], store=None)
    # Temperature should NOT be set for Responses API models
    assert "temperature" not in out
    assert out["tool_choice"] == "auto"
    # include should contain original and encrypted_content
    assert set(out["include"]) >= {"text.output_text", "reasoning.encrypted_content"}
    # store default to False when None passed
    assert out["store"] is False
    # reasoning config defaulted
    r = out["reasoning"]
    assert r["effort"] in {"low", "medium", "high", "none"}
    assert r["summary"] == "detailed"
    # max_output_tokens preserved
    assert out["max_output_tokens"] == 256


def test_responses_kwargs_default_no_temperature():
    """Test that by default, temperature is not included in kwargs for Responses API."""
    llm = LLM(model="gpt-5-mini-2025-08-07")
    llm.max_output_tokens = 512

    # Call with empty user_kwargs - should not include temperature
    out = select_responses_options(llm, {}, include=None, store=None)

    # Verify temperature is NOT in the output
    assert "temperature" not in out, (
        "Temperature should not be set by default for Responses API"
    )

    # Verify other expected defaults are present
    assert out["tool_choice"] == "auto"
    assert out["max_output_tokens"] == 512
    assert "reasoning" in out


@patch("openhands.sdk.llm.llm.litellm_responses")
def test_llm_responses_end_to_end(mock_responses_call):
    # Configure LLM
    llm = LLM(model="gpt-5-mini")
    # messages: system + user
    sys = Message(role="system", content=[TextContent(text="inst")])
    user = Message(role="user", content=[TextContent(text="hi")])

    # Build typed ResponsesAPIResponse with usage
    msg = build_responses_message_output(["ok"])
    usage = ResponseAPIUsage(input_tokens=10, output_tokens=5, total_tokens=15)
    resp = ResponsesAPIResponse(
        id="r1",
        created_at=0,
        output=[msg],
        parallel_tool_calls=False,
        tool_choice="auto",
        top_p=None,
        tools=[],
        usage=usage,
        instructions="inst",
        status="completed",
    )

    mock_responses_call.return_value = resp

    result = llm.responses([sys, user])
    # Returned message is assistant with text
    assert result.message.role == "assistant"
    assert [c.text for c in result.message.content if isinstance(c, TextContent)] == [
        "ok"
    ]
    # Telemetry should have recorded usage (one entry)
    assert len(llm._telemetry.metrics.token_usages) == 1  # type: ignore[attr-defined]
