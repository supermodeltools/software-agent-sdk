from abc import ABC, abstractmethod
from logging import getLogger

from openhands.sdk.context.view import View
from openhands.sdk.event.condenser import Condensation
from openhands.sdk.utils.models import (
    DiscriminatedUnionMixin,
)


logger = getLogger(__name__)


class CondenserBase(DiscriminatedUnionMixin, ABC):
    """Abstract condenser interface.

    Condensers take a list of `Event` objects and reduce them into a potentially smaller
    list.

    Agents can use condensers to reduce the amount of events they need to consider when
    deciding which action to take. To use a condenser, agents can call the
    `condensed_history` method on the current `State` being considered and use the
    results instead of the full history.

    If the condenser returns a `Condensation` instead of a `View`, the agent should
    return `Condensation.action` instead of producing its own action. On the next agent
    step the condenser will use that condensation event to produce a new `View`.
    """

    @abstractmethod
    def condense(self, view: View) -> View | Condensation:
        """Condense a sequence of events into a potentially smaller list.

        New condenser strategies should override this method to implement their own
        condensation logic. Call `self.add_metadata` in the implementation to record any
        relevant per-condensation diagnostic information.

        Args:
            view: A view of the history containing all events that should be condensed.

        Returns:
            View | Condensation: A condensed view of the events or an event indicating
            the history has been condensed.
        """

    def handles_condensation_requests(self) -> bool:
        """Whether this condenser handles explicit condensation requests.

        If this returns True, the agent will trigger the condenser whenever a
        CondensationRequest event is added to the history. If False, the condenser will
        only be triggered when the agent's own logic decides to do so (e.g. context
        window exceeded).

        Returns:
            bool: True if the condenser handles explicit condensation requests, False
            otherwise.
        """
        return False

    # Shared token-budget utilities (for condenser authors)
    @staticmethod
    def compute_token_budget(llm, token_margin_ratio: float) -> int | None:
        """Compute usable input-token budget for a target LLM.

        Returns an integer budget (>= 0) or None if limits are unknown.
        """
        try:
            max_input = getattr(llm, "max_input_tokens", None)
            if not max_input:
                return None
            max_output = getattr(llm, "max_output_tokens", 0) or 0
            headroom = int(max_input * token_margin_ratio)
            return max(0, int(max_input) - int(max_output) - headroom)
        except Exception:
            return None

    @staticmethod
    def estimate_token_count(llm, events) -> int:
        """Estimate tokens for a sequence of LLMConvertibleEvent using the given LLM.

        Falls back to 0 on failure.
        """
        try:
            from openhands.sdk.event.base import LLMConvertibleEvent

            messages = LLMConvertibleEvent.events_to_messages(list(events))
            return int(llm.get_token_count(messages))
        except Exception:
            return 0

    @staticmethod
    def max_tail_within_budget(view: View, llm, keep_first: int, budget: int) -> int:
        """Binary-search the longest tail we can keep under the token budget.

        Counts tokens using the provided LLM. The head is fixed to keep_first.
        Returns number of tail events to keep (>= 0).
        """
        from openhands.sdk.event.base import LLMConvertibleEvent

        head = view[:keep_first]
        total_len = len(view)
        max_tail_possible = max(0, total_len - keep_first)
        low, high = 0, max_tail_possible
        best = 0
        while low <= high:
            mid = (low + high) // 2
            kept_events = list(head) + (list(view[-mid:]) if mid > 0 else [])
            msgs = LLMConvertibleEvent.events_to_messages(kept_events)
            try:
                t = int(llm.get_token_count(msgs))
            except Exception:
                # If counting fails, be conservative and stop expanding
                high = mid - 1
                continue
            if t <= budget:
                best = mid
                low = mid + 1
            else:
                high = mid - 1
        return best


class PipelinableCondenserBase(CondenserBase):
    """Abstract condenser interface which may be pipelined. (Since a pipeline
    condenser should not nest another pipeline condenser)"""


class RollingCondenser(PipelinableCondenserBase, ABC):
    """Base class for a specialized condenser strategy that applies condensation to a
    rolling history.

    The rolling history is generated by `View.from_events`, which analyzes all events in
    the history and produces a `View` object representing what will be sent to the LLM.

    If `should_condense` says so, the condenser is then responsible for generating a
    `Condensation` object from the `View` object. This will be added to the event
    history which should -- when given to `get_view` -- produce the condensed `View` to
    be passed to the LLM.
    """

    @abstractmethod
    def should_condense(self, view: View) -> bool:
        """Determine if a view should be condensed."""

    @abstractmethod
    def get_condensation(self, view: View) -> Condensation:
        """Get the condensation from a view."""

    def condense(self, view: View) -> View | Condensation:
        # If we trigger the condenser-specific condensation threshold, compute and
        # return the condensation.
        if self.should_condense(view):
            return self.get_condensation(view)

        # Otherwise we're safe to just return the view.
        else:
            return view
